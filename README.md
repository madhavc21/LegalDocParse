# PDF Document Ingestion Pipeline

This project implements a Python-based ingestion pipeline to parse PDF documents, extract structured content, and derive legal metadata. It exposes an HTTP API endpoint for processing PDF uploads.

## Features

- **PDF Content Parsing:** Extracts paragraphs, lists, tables (as HTML), figures (captions, image filenames), headers, and footers with page numbers.
- **Legal Metadata Extraction:** Identifies and extracts:
    - Document name and primary date.
    - All other dates mentioned with surrounding context.
    - References to letters, laws, clauses, articles, acts, and legal precedents (case law).
    - Persons mentioned in the document.
- **HTTP API:** A `/ingest` POST endpoint for uploading PDF files.
- **Output:** Saves processed data (content + metadata) to a JSON file and returns its path.

## Tech Stack

- Python 3.11+
- FastAPI: For the web API.
- Uvicorn: ASGI server for FastAPI.
- Docling: For PDF parsing and conversion to HTML.
- BeautifulSoup4 & lxml: For parsing HTML content.
- SpaCy: For Natural Language Processing (NER).
    - `en_core_web_lg`: General English NER model (for Persons).
    - `en_legal_ner_trf` (from OpenNyAI): Legal-specific NER model (for legal entities and dates).
- python-dateparser: For robust date string parsing.

## Project Structure

```
.
|-- main.py                   # FastAPI application
|-- document_parser.py        # PDF content extraction logic
|-- metadata_extractor.py     # Legal metadata extraction logic
|-- requirements.txt          # Python dependencies
|-- Dockerfile                # For containerizing the application
|-- README.md                 # This file
|-- uploaded_pdfs/            # (Created at runtime) Directory for temporary PDF uploads
|-- processed_outputs/        # (Created at runtime) Directory for final JSON outputs
|-- temp_processing_workspace/ # (Created at runtime) For intermediate Docling assets
```

## Setup and Running Locally

### Prerequisites

- Python 3.11 or higher
- Pip (Python package installer)

### 1. Clone the Repository (if applicable)

```bash
git clone https://github.com/madhavc21/LegalDocParse
cd LegalDocParse
```

### 2. Create a Virtual Environment (Recommended)

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Download SpaCy Models

The base `spacy` library is installed via `requirements.txt`. You then need to download the specific models:

```bash
pip install  https://huggingface.co/ali6parmak/en_legal_ner_trf/resolve/main/en_legal_ner_trf-3.2.0-py3-none-any.whl
```

### 5. Run the Application

```bash
python main.py
```
The API will typically be available at `http://127.0.0.1:8000`.

### 6. Using the API

- Open your browser and navigate to `http://127.0.0.1:8000/docs` for the interactive Swagger UI.
- Use the `/ingest` endpoint to upload a PDF file.
- The API will respond with a JSON message indicating success and the path to the generated output file containing extracted content and metadata. Output files are saved in the `processed_outputs/` directory.
- A `/health` endpoint is available at `http://127.0.0.1:8000/health` to check API status.

## Running with Docker

### Prerequisites

- Docker installed and running.

### 1. Build the Docker Image

Navigate to the project's root directory (where the `Dockerfile` is located) and run:

```bash
docker build -t pdf-ingestion-pipeline .
```

### 2. Run the Docker Container

```bash
docker run -p 8000:8000 \
       -v $(pwd)/processed_outputs:/app/processed_outputs \
       pdf-ingestion-pipeline
```

**Explanation of the `docker run` command:**
- `-p 8000:8000`: Maps port 8000 of the host to port 8000 of the container.
- `-v $(pwd)/processed_outputs:/app/processed_outputs`: Mounts the local `processed_outputs` directory into the container at `/app/processed_outputs`. This allows you to access the JSON output files generated by the application directly on your host machine. Create the `processed_outputs` directory locally if it doesn't exist before running.

The API will then be accessible at `http://localhost:8000`.

**Note on GPU usage with Docker:**
The provided `Dockerfile` installs CPU versions of PyTorch for wider compatibility. If you intend to run this on a host with an NVIDIA GPU and have the NVIDIA Docker toolkit installed, you can modify the `Dockerfile` to install GPU-enabled PyTorch (see comments in Dockerfile) and run the container with GPU support (e.g., `docker run --gpus all ...`). This will significantly speed up the legal NER model.

## Approach and Design Decisions

- **PDF Parsing (Content Extraction):**
    - `Docling` library is used to convert PDF documents into HTML. This approach was chosen because Docling can handle complex PDF structures and preserve some layout information.
    - The generated HTML is then parsed using `BeautifulSoup` to extract elements like paragraphs, headers, lists, tables (as HTML strings), and figure references. Page breaks are inferred from specific HTML markers inserted by Docling.
- **Metadata Extraction:**
    - A hybrid approach is used:
        - **SpaCy for NER:**
            - `en_legal_ner_trf` (a transformer model) for specialized legal entities such as Statutes (Acts), Provisions (Clauses, Articles, Sections), Precedents (Case Law), Personnels and Dates. This model provides higher accuracy for legal domain text.
        - **`python-dateparser`:** Used to parse date strings identified by the NER model into standardized datetime objects.
        - **Regex:** Employed for specific patterns not well-covered by NER, such as references to letters/correspondence.
    - The strategy for dates prioritizes the `DATE` entities found by `en_legal_ner_trf` due to its higher precision on legal documents compared to broader date parsing tools applied to raw text.
- **API:**
    - `FastAPI` is chosen for its performance, ease of use, automatic data validation, and built-in interactive API documentation (Swagger UI).
    - The `/ingest` endpoint handles PDF uploads, orchestrates the parsing and metadata extraction steps, and saves the combined output.
- **Modularity:** The code is structured into:
    - `document_parser.py`: For PDF-to-structured content.
    - `metadata_extractor.py`: For extracting metadata from the structured content.
    - `main.py`: For the FastAPI application and endpoint logic.

## Assumptions Made

- PDFs are text-based or have a good quality OCR layer if scanned. The current pipeline does not include an explicit OCR step if `docling` itself doesn't handle it.
- The `en_legal_ner_trf` model provides reasonable coverage for the types of legal entities expected.
- The primary document date heuristic (earliest relevant date on first few pages) is a starting point and may need refinement for specific document types or user requirements.
- Output JSON files are saved locally; no external storage or database is used in this version.

## Limitations

- **Document Date Heuristic:** The automatic determination of the primary "document_date" is based on simple heuristics and might not be accurate for all documents. This is a known area for potential improvement.
- **Complex Table Structures:** While tables are extracted as HTML, very complex nested tables or tables with unusual formatting might not be perfectly represented.
- **Figure Caption Linkage:** Figure captions are extracted if present near a `figure` tag in HTML, but complex layouts might make precise caption-to-image linkage challenging.
- **OCR Quality:** If PDFs are image-based, the quality of extraction depends heavily on the OCR performance of the underlying `docling` library or the PDF itself. No separate OCR step is explicitly added in this pipeline.
- **Scalability for Concurrent Ingestion:** The current API processes one PDF at a time. For high concurrency, a task queue (like Celery with Redis/RabbitMQ) and horizontally scalable workers would be needed.
- **Resource Intensive Models:** The transformer-based legal NER model (`en_legal_ner_trf`) is resource-intensive. Performance will be significantly better with a GPU.
- **Speed with large document ingestion:** Conversion of very large docs (`RFD.pdf`) is proportionally slower, thus a GPU enviornment is HIGHLY recommended. CPU times can become very large.